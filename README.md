# pyPhraseSuggesting
Backend implementation for autocomplete form fields.
Поиск окончания фразы. Для автозаполнения в текстовых полях форм.

По введенной строке (предполагается, что последнее слово не окончено) 
ищет несколько наиболее вероятных фраз с оконченным последним словом.

В качества источника фраз используем корпус лента.ру. В последствии надо 
сделать обучающийся механизм, который будет добавлять в словарь фразы, 
введенные самими пользователями в полях формы.

В подборе базируюсь на эту теорию:
https://habr.com/ru/post/675218/

1) Формируем униграмы и биграммы с кол-вом включений, используя паттер map-reduce (mr4mp). 
	зы: вылезла MemoryError при попытке весь корпус загрузить в 24 потока, 
	если убрать колво потоков до 6-8 - всё нормально.

2) Загружем в базу в две коллекции
	1) по униграмам с индексом, по которому будем искать на полное совпадение или по префиксу (/^prefi/) 
		и иметь быструю сортировку по кол-ву включений
	2) по биграмам (используем вместо текста идентификаторы униграм?), индексы по обоим полям
		для поиска и кол-во для сортировки

Так ищем варианты:
На входе: фраза состоящая из введенных слов(или ничего, заканчивается пробелом), 
	последнего неполного слова (или ничего, не заканчивается пробелом).
1) Вычищам, приводим и разбиваем фразу на части - массив слов, последнее неполное слово.
2) Подбираем наивероятнейшие точные варианты, проход вперёд, 
	когда слева всё определено и фиксированно, 
	а значит не влияет на правую часть дальше одного шага:

	1) Ищем по неполному слову в униграммах как префикс. Берём топ, например 10 по каунтам.
	2) Ищем по последнему полному слову в униграммах, нужны каунты.
	3) Ищем в биграмах по последнему слову и любому следующему слову из первого пункта.
	4) Для каждого варианта неполного слова ищем биграмы, где это слово слева. Берём топ, например 10.
	5) Делаем 4 пункт пока:
		- не начнется повтор слов в цепочках
		- не дойдём до "_" (конца)
		- не закончатся варианты
		- не достигнем глубины 4
		- вероятности не станут совсем мизерными (порог по медиане уже имеющихся, например)

	6) Считаем вероятности найденных цепочек, ранжируем

3) Подбираем наивероятнейшие точные варианты, проход назад,
	когда при введенной фразе предполагаем, что это окончание
	более длинной фразы.

	1) Ищем все полные слова в униграммах, нужны каунты и ид.
	2) Ищем для первого слова биграммы наиболее вероятные для него, как второго слова. Берём топ 10.
		Каунты для этого не подойдут, видимо для этого надо предварительно, 
		при загрузке, считать вероятность.
	3) Повторяем пока:
		- не начнеся повтор
		- не дойдём до _
		- не достигнем глубины 4
		- вероятности не станут совсем мизерными (порог по медиане уже имеющихся, например)


